%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%This is the LaTeX ARTICLE template for RSC journals
%Copyright The Royal Society of Chemistry 2016
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[twoside,twocolumn,9pt]{article}
\usepackage{extsizes}
\usepackage[super,sort&compress,comma]{natbib} 
\usepackage[version=3]{mhchem}
\usepackage[left=1.5cm, right=1.5cm, top=1.785cm, bottom=2.0cm]{geometry}
\usepackage{balance}
\usepackage{mathptmx}
\usepackage{sectsty}
\usepackage{graphicx} 
\usepackage{lastpage}
\usepackage[format=plain,justification=justified,singlelinecheck=false,font={stretch=1.125,small,sf},labelfont=bf,labelsep=space]{caption}
\usepackage{float}
\usepackage{fancyhdr}
\usepackage{fnpos}
\usepackage[english]{babel}
\addto{\captionsenglish}{%
  \renewcommand{\refname}{Notes and references}
}
\usepackage{array}
\usepackage{droidsans}
\usepackage{charter}
\usepackage[T1]{fontenc}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{setspace}
\usepackage[compact]{titlesec}
\usepackage{hyperref}
%%%Please don't disable any packages in the preamble, as this may cause the template to display incorrectly.%%%


% \usepackage{epstopdf}%This line makes .eps figures into .pdf - please comment out if not required.
% self-defined imports
\usepackage{tabularray}
\usepackage{booktabs}
\usepackage[inter-unit-product =\cdot]{siunitx}
\usepackage{stfloats}
\graphicspath{ {./img/} }


\definecolor{cream}{RGB}{222,217,201}

%%%%%%%%% Preamble of the bibliography, can be commented or deleted 
% \def\bibpreamble{For the reference section, the style file \texttt{rsc.bst} can be used to generate the correct reference style.\footnotemark[4]
% \begin{enumerate}
% \item{Citations should appear here in the format A. Name, B. Name and C. Name, \emph{Journal Title}, 2000, \textbf{35}, 3523;} 
% \item{A. Name, B. Name and C. Name, \emph{Journal Title, 2000}, \textbf{35}, 3523.}
% \end{enumerate}
% ... \\\\
% We encourage the citation of primary research over review articles, where appropriate, in order to give credit to those who first reported a finding. \href{https://www.rsc.org/news-events/articles/2020/jun/rsc-signs-dora/}{Find out more about our commitments to the principles of San Francisco Declaration on Research Assessment (DORA).}}
%%%%%%%%% 

\begin{document}

\pagestyle{fancy}
\thispagestyle{plain}
\fancypagestyle{plain}{
%%%HEADER%%%
\renewcommand{\headrulewidth}{0pt}
}
%%%END OF HEADER%%%

%%%PAGE SETUP - Please do not change any commands within this section%%%
\makeFNbottom
\makeatletter
\renewcommand\LARGE{\@setfontsize\LARGE{15pt}{17}}
\renewcommand\Large{\@setfontsize\Large{12pt}{14}}
\renewcommand\large{\@setfontsize\large{10pt}{12}}
\renewcommand\footnotesize{\@setfontsize\footnotesize{7pt}{10}}
\makeatother

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\renewcommand\footnoterule{\vspace*{1pt}% 
\color{cream}\hrule width 3.5in height 0.4pt \color{black}\vspace*{5pt}} 
\setcounter{secnumdepth}{5}

\makeatletter 
\renewcommand\@biblabel[1]{#1}            
\renewcommand\@makefntext[1]% 
{\noindent\makebox[0pt][r]{\@thefnmark\,}#1}
\makeatother 
\renewcommand{\figurename}{\small{Fig.}~}
\sectionfont{\sffamily\Large}
\subsectionfont{\normalsize}
\subsubsectionfont{\bf}
\setstretch{1.125} %In particular, please do not alter this line.
\setlength{\skip\footins}{0.8cm}
\setlength{\footnotesep}{0.25cm}
\setlength{\jot}{10pt}
\titlespacing*{\section}{0pt}{4pt}{4pt}
\titlespacing*{\subsection}{0pt}{15pt}{1pt}
%%%END OF PAGE SETUP%%%

%%%FOOTER%%%
\fancyfoot{}
\fancyfoot[LO,RE]{\vspace{-7.1pt}\includegraphics[height=9pt]{head_foot/LF}}
\fancyfoot[CO]{\vspace{-7.1pt}\hspace{13.2cm}\includegraphics{head_foot/RF}}
\fancyfoot[CE]{\vspace{-7.2pt}\hspace{-14.2cm}\includegraphics{head_foot/RF}}
\fancyfoot[RO]{\footnotesize{\sffamily{1--\pageref{LastPage} ~\textbar  \hspace{2pt}\thepage}}}
\fancyfoot[LE]{\footnotesize{\sffamily{\thepage~\textbar\hspace{3.45cm} 1--\pageref{LastPage}}}}
\fancyhead{}
\renewcommand{\headrulewidth}{0pt} 
\renewcommand{\footrulewidth}{0pt}
\setlength{\arrayrulewidth}{1pt}
\setlength{\columnsep}{6.5mm}
\setlength\bibsep{1pt}
%%%END OF FOOTER%%%

%%%FIGURE SETUP - please do not change any commands within this section%%%
\makeatletter 
\newlength{\figrulesep} 
\setlength{\figrulesep}{0.5\textfloatsep} 

\newcommand{\topfigrule}{\vspace*{-1pt}% 
\noindent{\color{cream}\rule[-\figrulesep]{\columnwidth}{1.5pt}} }

\newcommand{\botfigrule}{\vspace*{-2pt}% 
\noindent{\color{cream}\rule[\figrulesep]{\columnwidth}{1.5pt}} }

\newcommand{\dblfigrule}{\vspace*{-1pt}% 
\noindent{\color{cream}\rule[-\figrulesep]{\textwidth}{1.5pt}} }

\makeatother
%%%END OF FIGURE SETUP%%%

%%%TITLE, AUTHORS AND ABSTRACT%%%
\twocolumn[
  \begin{@twocolumnfalse}
{\includegraphics[height=30pt]{head_foot/journal_name}\hfill\raisebox{0pt}[0pt][0pt]{\includegraphics[height=55pt]{head_foot/RSC_LOGO_CMYK}}\\[1ex]
\includegraphics[width=18.5cm]{head_foot/header_bar}}\par
\vspace{1em}
\sffamily
\begin{tabular}{m{4.5cm} p{13.5cm} }

\includegraphics{head_foot/DOI} & \noindent\LARGE{\textbf{Performance of upcycled tyre-derived carbon in ethaline-glycol ionic electrolytes for sustainable supercapacitors: modelling cyclic voltammetry using standard and machine-learning methods.}} \\%Article title goes here instead of the text "This is the title"
\vspace{0.3cm} & \vspace{0.3cm} \\

 & \noindent\large{Maria A. Sandoval-Riofrio,\textit{$^{1, a}$} and Kavisha Jayathunge\textit{$^{2, a}$}} \\%Author names go here instead of "Full name", etc.

\includegraphics{head_foot/dates} & \noindent\normalsize{This study explores the development of sustainable supercapacitors using electrochemically activated carbon derived from plastic and tyre waste as electrode materials and ethaline, a deep eutectic solvent, as a green electrolyte. The electrochemical molten salts (EMS) process was employed to activate black carbon, a waste byproduct from tyre recycling, resulting in significant enhancements in carbon properties, including a 39.4\% increase in BET surface area (from \SI{43.1}{\meter\square\per\gram} to \SI{60.09}{\meter\square\per\gram}) and improved pore volume and size distribution. These findings establish the EMS process as a scalable and effective method for producing high-performance porous carbon materials. Supercapacitors assembled with EMS-activated carbon and ethaline exhibited remarkable electrochemical performance, achieving a specific capacitance of \SI{210.56}{\farad\per\gram}, surpassing many conventional aqueous and organic systems. Combined CV and EIS analyses revealed primarily capacitive behaviour with notable contributions from diffusion processes, particularly at elevated temperatures. Specific capacitances of 166.14, 121.63, and \SI{69.81}{\farad\per\gram} at \SI{45}{\degreeCelsius}, \SI{65}{\degreeCelsius}, and \SI{90}{\degreeCelsius}, respectively, underscored the thermal sensitivity and ionic mobility of ethaline. These results highlight the potential of integrating waste-derived carbon materials with environmentally friendly electrolytes to create efficient, sustainable energy storage systems. Further studies are recommended to optimise electrolyte performance and address decomposition mechanisms for broader applications.} \\%The abstrast goes here instead of the text "The abstract should be..."

\end{tabular}

 \end{@twocolumnfalse} \vspace{0.6cm}

  ]
%%%END OF TITLE, AUTHORS AND ABSTRACT%%%

%%%FONT SETUP - please do not change any commands within this section
\renewcommand*\rmdefault{bch}\normalfont\upshape
\rmfamily
\section*{}
\vspace{-1cm}


%%%FOOTNOTES%%%

\footnotetext{\textit{$^{1}$~Department of Engineering; E-mail: msandoval2@bournemouth.ac.uk}}
\footnotetext{\textit{$^{2}$~National Centre for Computer Animation; E-mail: kjayathunge@bournemouth.ac.uk}}
\footnotetext{\textit{$^{a}$~Talbot Campus, Bournemouth University, Fern Barrow, Poole BH12 5BB, UK}}

%Please use \dag to cite the ESI in the main text of the article.
%If you article does not have ESI please remove the the \dag symbol from the title and the footnotetext below.
%\footnotetext{\dag~Supplementary Information available: [details of any supplementary information available should be included here]. See DOI: 00.0000/00000000.}
%additional addresses can be cited as above using the lower-case letters, c, d, e... If all authors are from the same address, no letter is required

% \footnotetext{\dag~Additional footnotes to the title and authors can be included \textit{e.g.}\ `Present address:' or `These authors contributed equally to this work' as above using the symbols: \ddag, \textsection, and \P. Please place the appropriate symbol next to the author's name and include a \texttt{\textbackslash footnotetext} entry in the the correct place in the list.}


%%%END OF FOOTNOTES%%%

%%%MAIN TEXT%%%%
\section{Deep learning analysis}
\label{sec:ml_analysis}
Machine learning models can be thought of as universal function approximators. That is, provided enough data to learn from, they are able to model any continuous, closed domain function to an arbitrary degree of accuracy~\cite{geuchenUniversalApproximationComplexvalued2025}. Such models consist of interconnected ``neurons'' arranged in layers. These connections are dense, meaning each neuron is connected to all other neurons in the previous and next layer, giving rise to the common ``fully connected'' layer terminology. We will refer to a single layer as $F_{i}^{n,m}$ where $i$ is the position of the layer, $n$ is the input size (the number of input neurons expected), and $m$ is the output size. The layer computes:

\begin{equation}
\hat{y} = F_{i}^{n, m}(x) = W_{i}x + b_{i}
\end{equation}


where $x$ is an n-dimensional vector, $\hat{y}$ is an m-dimensional vector, $W_i$ is an $n \times m$ weight matrix and $b_i$ is a constant term.  Layers may be chained sequentially, as long as the inner dimensions of adjacent layers are equal, i.e. $W_i$ and $W_{i+1}$ can be multiplied. Such a sequence of connected layers is commonly referred to as a Multilayer Perceptron (MLP). An important element of the layers of the MLP is the activation function, which is applied to the output of each neuron, and introduces non-linearity into the model. This is essential for an MLP to fit the definition of a universal function approximator as defined earlier, because any sequence of fully connected layers must necessarily collapse into a single linear transform, which is not powerful enough to represent more complicated functions. The Rectified Linear Unit function (ReLU) is a popular activation function~\cite{agarapDeepLearningUsing2019} designed for this purpose and is defined as follows:

\begin{equation}
\text{ReLU}(x) = \begin{cases}
  0 & x \leq 0 \\
  x & x > 0
\end{cases}
\end{equation}

Thus, the MLP must have a ReLU activation function between each hidden layer, giving the final model structure as laid out in Figure~\ref{fig:model_arch}. Obtaining a $\hat{y}$ prediction from an MLP completes the ``forward pass'' part of training. The prediction is then evaluated against the target value $y$ (also called the ground truth value) using a cost function $L$ -- see Equations~\ref{eq:loss_current}~-~\ref{eq:loss} for how it is defined for this application. The partial derivative of $L$ is calculated with respect to each parameter $w_{i}^{n,m}$ in $W_i$: 

\begin{figure*}
 \centering
 \includegraphics[width=\linewidth]{model_arch.png}
 \caption{An overview of the training pipeline and model architecture, incorporating dropout after the largest layer to discourage overfitting. The input is a 6-dimensional vector consisting of a potential step, scan direction, and several experimental conditions under which the electrode material was sysnthesised; the output is a prediction of the current value at that potential. Four trainable weights $W^{4}$ provide a scaling factor for each of the conditions. Ground truth hysteresis curves shown in blue and model prediction shown in orange.}
 \label{fig:model_arch}
\end{figure*}

\begin{equation}
\frac{\partial L}{\partial w_{i}^{n,m}} = \frac{\partial L }{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial w_{i}^{n,m}}
\end{equation}

Computing all such partials completes the ``backwards'' pass of the training step -- a process known as backpropagation --  and gives us $\frac{\partial L}{\partial W_i}$, which tells the model how much, and the direction in which $W$ should change in order to minimise $L$. Repeating these forward and backwards passes over the MLP over and over again brings the weights closer to their ``ideal'' values -- ones for which $L$ is minimised. In other words, the optimsing algorithm uses the difference between the output of the model and its target to update $W$ in the correct direction. This is what is meant by training a machine learning model.  

\subsection{Previous work}
The application of machine learning systems to the prediction of specific capacitance values is varied -- some previous investigations involve predicting specific capacitance directly from experimental conditions~\cite{kumaryogeshMachineLearningApproach2025}, while others use these experimental conditions to predict hysteresis \emph{\textbf{Note to Alex: Is this the correct terminology? Are CV curves hysteresis loops?}} curves~\cite{ravichandranMachineLearningBasedPrediction2024}; the latter approach allows the indirect calculation of specific capacitance $C_{sp}$, which is proportional to the area between charge and discharge curves. It is this latter approach that is used by the present study, because predicting the full charge-discharge dynamics of cyclic voltammetry allows the model to generalise better to a wider range of conditions~\cite{deebansokCapacitiveTendencyConcept2024} -- especially important given the limited data available for this study. Using this approach, $C_{sp}$ is given by:

\begin{equation}
  \label{eq:specific_cap}
  C_{sp} = \frac{\int I_{pred}(E) \,dE}{ 2 \cdot \nu \cdot m \cdot \Delta V}
\end{equation}


where $I_{pred}$ is the function that is approximated by the MLP, $E$ is the potential, $\nu$ is the scanrate, m is the total mass of the active material in the electrodes, and $\Delta V$ is the change in voltage over the whole charge/discharge cycle.

\subsection{Dataset}
The complete dataset is made up of current samples from 5 cyclic voltammetry cycles -- see Table~\ref{tab:dataset}. These were collected via potentiostat with a scanrate of \SI{100}{\milli\volt\per\second}. Each cycle consists of 264 rows, with columns for current (I / \unit{\ampere}), voltage step (E / \unit{\volt}), active electrode mass (M / \unit{\milli\gram}), temperature (T / \unit{\degreeCelsius}), electrolysis duration (D / \unit{\hour}), electrolysis voltage ($V_e$ / \unit{\volt}), and scan direction (S). The last feature tells the model if the particular point is on the charge or discharge curve and is represented by either +1 (charging)  or -1 (discharging).  

\begin{table}
\caption{Characteristics of each cycle that constitutes the dataset used for training our model. The scanrate was \SI{100}{\milli\volt\per\second} in all cases.}
\label{tab:dataset}
\small
\centering
\begin{tblr}{
  hline{1-2} = {-}{}
}
Cycle      & M / mg  & T / $^{\circ}$C& D / h & $V_e$ / V & Electrolyte \\
BCMS2      & 0.11    & 800            & 2     & 2         & Ethaline    \\
BCMS3      & 0.11    & 750            & 2     & 2         & Ethaline    \\
BCMS5      & 0.18    & 650            & 2     & 2         & Ethaline    \\
BCMS7      & 0.25    & 700            & 2     & 2.8       & Ethaline    \\
BCMS9      & 0.25    & 750            & 2     & 2.8       & Ethaline       
\end{tblr}
\end{table}

Notice that the various inputs span many orders of magnitude -- this is a problem for machine learning models. Using raw features such as those presented in Table~\ref{tab:dataset} mean that ones with large absolute values dominate the weights of the model, even though they may not be as important, and vice versa, which can lead to lower performance~\cite{kimInvestigatingImpactData2025}. For this reason, all features are scaled across all cycles such that they have zero mean and unit standard deviation. To transform an individual instance of a feature $d_i$ (e.g. electrode mass, temperature) to its scaled value $d_{norm,i}$:

\begin{equation}\label{eq:norm}
  d_{norm,i} = \frac{d_i - \mu(D)}{\sigma(D)}
\end{equation}

where $D$ is the collection of all the values for this feature, $\mu$ is the mean of values in $D$, and $\sigma$ is their standard deviation. These scaled values are used in all subsequent machine learning operations, and final outputs are de-normalised using the inverse of Equation~\ref{eq:norm} for display and comparison with experimental values. Current values were also transformed in this way. However, the potential step $E$ was rescaled slightly differently: rather than ensuring zero mean and unit variance, we rescale them such that the minimum potential across the whole dataset is -1 and the maximum is +1:

\begin{align}
  E_{mid} &= \frac{E_{max} + E_{min}}{2} \nonumber \\
  E_{half-range} &= \frac{E_{max} - E_{min}}{2} \nonumber \\
  E_{norm} &= \frac{E - E_{mid}}{E_{half-range}}
\end{align}

where $E_{min}$ and $E_{max}$ are the minimum and maximum potential values \emph{across the whole dataset}. This scaling ensures that all cycles share a common and bounded potential domain, making it easier for the model to lear consistent relationships across different experimental conditions.

The input to the model is therefore a 6 dimensional, normalised feature vector that consists of a potential step and experimental conditions under which the electrode material was synthesised, and the output is the current value at the potential step.  

\subsection{Experimental design and model architecture}
\label{subsec:arch}
We employ leave-one-out cross validation when training: for any one run, the model is trained only on points from 4 of the cycles; points from the 5$^{th}$ are held out for validation. Here, validation means only evaluating the forward pass of the model and not updating the weights of the model. It is a test to determine if the model is able to generalise to ``unseen'' data, and each cycle is subjected to this treatment in turn. We repeat training runs five times independently to build confidence that the model's output is reliable.

Due to the small size of the dataset, additional ... is introduced to improve the training stability and to encourage the model to learn meaningful relationships between experimental conditions and the voltammetric response. To this end, we introduce four trainable parameters that act as scaling factors for the experimental conditions that are used when synthesising the black carbon -- active mass (M), electrolysis temperature (T), electrolysis duration (D) and electrolysis voltage ($V_e$). These parameters allow the model to explicitly learn the relative importance of each condition rather than solely relying on implicit weighting within the first layer of the MLP. 

As illustrated in Figure~\ref{fig:model_arch}, the learned scaling parameters are applied to the condition vector prior to concatenation with the potential step (E) and scan direction (S), resulting in the 6-dimensional input that is expected by the MLP. The learned weights are first divided by a temperature parameter $\tau$, which controls the sharpness of the relative weighting, and subsequently normalised by a softmax function to give the scaling factor. The scaling factor $s_i$ for each learned weight is given by:

\begin{equation}
  \label{eq:softmax}
  s_i = \frac{\exp(\alpha_i / \tau)}{\sum_{j=1}^{4} \exp(\alpha_j / \tau)}
\end{equation}

where $\alpha_i$ is any one of the four learned weights. The resulting normalised weights are multiplied element-wise by the condition vector, yielding the weighted condition vector.

The rest of the model is a multilayered perceptron with 4 hidden layers in between a 6-neuron input layer and a 1-neuron output layer. Please refer to Figure~\ref{fig:model_arch} for the full model architecture. Note the dropout layer, which randomly (with some probability $p$) deactivates a subset of neurons during training. This is done so that the model does not rely too heavily on any single neuron or set of neurons, thereby reducing overfitting and improving generalisation to new data~\cite{mouDropoutTrainingDatadependent2018}. Wider layers with more neurons benefit more from dropout as they are more at risk from overfitting~\cite{srivastavaDropoutSimpleWay2014}, hence the placement between $F_{2}$ and $F_{3}$.

As mentioned in the Section~\ref{sec:ml_analysis}, the objective function scores the output of the MLP so it can be used in gradient calculations, which in turn are used to inform updates to the model's weights. Because the model predicts the current at a particular voltage step, part of the loss function is the mean squared error (MSE) between the predicted and actual value. This is given by:

\begin{figure*}
 \centering
 \includegraphics[width=\linewidth]{results.png}
 \caption{A plot reporting the average normalised $C_{sp}$ over 5 runs per cycle per experimental setting. Error bars indicate 95\% confidence intervals over runs.}
 \label{fig:results}
\end{figure*}

\begin{equation}
  \label{eq:loss_current}
  L_{curr} = \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i)^{2}
\end{equation}

where $\hat{y}_i$ is the predicted current value, $y_i$ is the true value, and $N$ is the number of predicted points. Further, in order to maintain a smooth curve that is physically consistent with the behaviour of real hysteresis curves, we impose a penalty on the second derivative of the predicted current with respect to potential, discouraging sharp local curvature and jitter:

\begin{equation}
  \label{eq:loss_smooth}
  L_{smth} =  \Bigl\lVert\frac{\partial^2 I_{pred}}{\partial E^2}\Bigr\rVert_{2}^{2}
\end{equation}

where $I_{pred}$ is the predicted current and $E$ is the potential. Finally, we also use the area of the predicted curve in another MSE loss calculation to encourage the model to produce the correct net area exhibited by the experimental curve:

\begin{equation}
  \label{eq:loss_area}
  L_{area} = \frac{1}{M} \sum_{j=1}^{M} (\hat{a}_j - a_j)^{2}
\end{equation}
where $M$ is the number of cycles, $a_j$ is the actual area between the curves for a given cycle, and $\hat{a}_j$ is the area calculated using predicted current values.  The losses are combined for final  objective for the MLP:
\begin{equation}
  \label{eq:loss}
  L = L_{curr} + L_{smth} + L_{area}
\end{equation}


In order to assess the importance of these loss functions, an ablation study was conducted to measure the impact of each objective on the final prediction. A comparison of $C_{sp}$ predicted by models under these experimental settings is given in Figure~\ref{fig:results}, and a comparison of their relative errors is given in Table~\ref{tab:errors}. The base MLP, henceforth called ``Base'' is only tasked with the $L_{curr}$ objective. The following experimental modifications were added to the Base model:
\begin{enumerate}
  \item  ``+ D'' -- inclusion of a \textbf{D}ropout layer, as discussed in Section~\ref{subsec:arch}
  \item  ``+ S'' -- addition of the \textbf{S}moothing $L_{smth}$ objective to $L$
  \item  ``+ A'' -- addition of the \textbf{A}rea $L_{area}$ objective to $L$ 
  \item  ``+ W'' -- inclusion of trainable \textbf{W}eights for each of the synthesis conditions 
\end{enumerate}


\subsection{Results and discussion}


\begin{table}
\caption{Average error (relative \%) for each cycle over 5 independent runs. Base+D+A exhibits the least overall error across all the settings.}
\label{tab:errors}
\small
\centering
\begin{tblr}{
  hline{1-2,7,8,9} = {-}{}
}
Cycle         & Base    & {Base\\+D}  & {Base\\+D\\+S}  & {Base\\+D\\+S\\+A}   & {Base\\+D\\+A} & {Base\\+D\\+A\\+W}\\
BCMS2         & 0.62    & 0.58        & 2.54            & 7.17                 & 0.77           & 3.25             \\
BCMS3         & 29.82   & 23.28       & 20.15           & 16.11                & 16.93          & 0.42             \\
BCMS5         & 7.04    & 15.05       & 7.88            & 2.61                 & 8.83           & 7.28             \\
BCMS7         & 30.87   & 25.37       & 18.50           & 26.78                & 17.92          & 4.69             \\
BCMS9         & 9.12    & 4.64        & 1.91            & 2.42                 & 0.39           & 11.46             \\
Avg. Err(\%)  & 15.49   & 13.79       & 10.20           & 11.02                & 8.97           & \textbf{5.42}
\end{tblr}
\end{table}


Results were collected for each experimental setting described in Section~\ref{subsec:arch}. Figure~\ref{fig:results} reports the averaged, normalised $C_{sp}$ values computed over five independent runs. Predicted $C_{sp}$ were normalised by their corresponding ground-truth values to facilitate comparison across settings, as $C_{sp}$ spans several orders of magnitude. Error bars indicate the 95\% confidence interval over the five runs. 

Overall, the Base+D+A+W configuration (corresponding to the model incorporating dropout, condition scaling, and the area loss of Equation~\ref{eq:loss_area}) exhibits the samllest deviation from the $C_{sp}$ baseline, on average.  This setting also yields comparatively narrower confidence intervals, suggesting improved stability across runs. Incorporating the area-based loss term resulted in improved performance across all curves. As shown in Figure~\ref{fig:results}, models that include this loss term tend to produce predictions that are closer to the target baseline. It should be noted that the smoothness loss term (Equation~\ref{eq:loss_smooth}) seems to be somewhat at odds with the area loss. This effect is reflected in  Table~\ref{tab:errors}, which presents the relative error percentage between the predicted and true $C_{sp}$ values across all the experimental settings. The addition of area loss to Base+D+S (10.2\% error) results in the error increasing slightly to 11.02\%. However, removing the smoothness loss (corresponding to the Base+D+A configuration) yields a lower overall error (8.97\%). This suggests that the two loss terms may impose competing constraints on the learned representations, which could partially limit their combined effectiveness. However, the most effective strategy incorporates condition scaling to the latter configuration (giving Base+D+A+W), and yielding an average error of 5.42\%.

\begin{table}
\caption{Relative weights assigned by the proposed model to the various molten salt parameters. The exclusion of one curve from the training gives a slightly different weight for each experiment, but overall, the model was most sensitive to temperature (T), followed by mass (M) and electrolysis voltage ($V_e$) on roughly equal footing. Electrolysis duration (D) was found to be the least important input in predicting $C_{sp}$.}
\label{tab:cond_weights}
\centering
\begin{tblr}{
  hline{1,2,7,8} = {-}{},
}
Experiment & M      & T      & D     & $V_e$ \\
BCMS2      & 0.350  & 0.288  & 0.161 & 0.201     \\
BCMS3      & 0.293  & 0.358  & 0.162 & 0.186     \\
BCMS5      & 0.270  & 0.351  & 0.070 & 0.309     \\
BCMS7      & 0.112  & 0.328  & 0.238 & 0.322     \\
BCMS9      & 0.197  & 0.580  & 0.090 & 0.134     \\
Average    & 0.244  & 0.381  & 0.144 & 0.230     
\end{tblr}
\end{table}

The inclusion of trainable scaling values for the four experimental conditions (M, T, V and D) offers insights into the relative importance of each of these in determining $C_{sp}$. Table~\ref{tab:cond_weights} demonstrates that on average, the model assigns most importance (38.1\% relative weight) to the temperature of the molten salt (T). Sensitivity to the active material mass (M) and electrolysis voltage ($V_e$) is roughly equal, at 24.4\% and 23.0\%, respectively. Finally, duration of electrolysis (D) recieves the lowest relative weight (14.4\%). This is unsuprising, as this setting did not vary at all  across the dataset (see Table~\ref{tab:dataset}), so could not explain any of the variance in the predicted output.

A key limitation of the experimental setup lies in the limited diversity of the input features. Although each of the five cycles contains 264 samples, all samples within a given cycle share the same experimental conditions; the only varying factor is the voltage step. Moreover, the voltage progression itself is identical across all cycles. Thus, even though a unique feature vector exists for each point, the dataset comprises only 5 unique condition vectors across 1320 samples. This extremely low condition variance substantially constrains the expressive capacity available to the model and limits the generalisability of the learned representations. In this context, the fact that the model exhibits meaningful performance at all is somewhat surprising, and underscores the need for more diverse experimental conditions (i.e., a wider range of these conditions) in future studies.

\section*{Conclusions}
The conclusions section should come in this section at the end of the article, before the Author contributions statement and/or Conflicts of interest statement.

\section*{Author contributions}
We strongly encourage authors to include author contributions and recommend using \href{https://casrai.org/credit/}{CRediT} for standardised contribution descriptions. Please refer to our general \href{https://www.rsc.org/journals-books-databases/journal-authors-reviewers/author-responsibilities/}{author guidelines} for more information about authorship.

\section*{Conflicts of interest}
In accordance with our policy on \href{https://www.rsc.org/journals-books-databases/journal-authors-reviewers/author-responsibilities/#code-of-conduct}{Conflicts of interest} please ensure that a conflicts of interest statement is included in your manuscript here.  Please note that this statement is required for all submitted manuscripts.  If no conflicts exist, please state that ``There are no conflicts to declare''.

\section*{Data availability}


A data availability statement (DAS) is required to be submitted alongside all articles. Please read our \href{https://www.rsc.org/journals-books-databases/author-and-reviewer-hub/authors-information/prepare-and-format/data-sharing/#dataavailabilitystatements}{full guidance on data availability statements} for more details and examples of suitable statements you can use.

\section*{Acknowledgements}

The acknowledgements come at the end of an article after the conclusions and before the notes and references. 

%%%END OF MAIN TEXT%%%

%The \balance command can be used to balance the columns on the final page if desired. It should be placed anywhere within the first column of the last page.

\balance

%If notes are included in your references you can change the title from 'References' to 'Notes and references' using the following command:
%\renewcommand\refname{Notes and references}

%%%REFERENCES%%%
\bibliography{refs} %You need to replace "rsc" on this line with the name of your .bib file
\bibliographystyle{rsc} %the RSC's .bst file
\end{document}
