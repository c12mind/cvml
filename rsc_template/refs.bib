@article{deebansokCapacitiveTendencyConcept2024,
  title = {Capacitive Tendency Concept alongside Supervised Machine-Learning toward Classifying Electrochemical Behavior of Battery and Pseudocapacitor Materials},
  author = {Deebansok, Siraprapha and Deng, Jie and Le Calvez, Etienne and Zhu, Yachao and Crosnier, Olivier and Brousse, Thierry and Fontaine, Olivier},
  year = 2024,
  month = feb,
  journal = {Nature Communications},
  volume = {15},
  number = {1},
  pages = {1133},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-024-45394-w},
  urldate = {2026-01-20},
  abstract = {In recent decades, more than 100,000 scientific articles have been devoted to the development of electrode materials for supercapacitors and batteries. However, there is still intense debate surrounding the criteria for determining the electrochemical behavior involved in Faradaic reactions, as the issue is often complicated by the electrochemical signals produced by various electrode materials and their different physicochemical properties. The difficulty lies in the inability to determine which electrode type (battery vs. pseudocapacitor) these materials belong to via simple binary classification. To overcome this difficulty, we apply supervised machine learning for image classification to electrochemical shape analysis (over 5500 Cyclic Voltammetry curves and 2900 Galvanostatic Charge-Discharge curves), with the predicted confidence percentage reflecting the shape trend of the curve and thus defined as a manufacturer. It's called ``capacitive tendency''. This predictor not only transcends the limitations of human-based classification but also provides statistical trends regarding electrochemical behavior. Of note, and of particular importance to the electrochemical energy storage community, which publishes over a hundred articles per week, we have created an online tool to easily categorize their data.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Batteries,Computational science,Electrochemistry,Energy storage},
  file = {/home/kavi/Zotero/storage/KT8RRMUN/Deebansok et al. - 2024 - Capacitive tendency concept alongside supervised machine-learning toward classifying electrochemical.pdf}
}

@article{geuchenUniversalApproximationComplexvalued2025,
  title = {Universal Approximation with Complex-Valued Deep Narrow Neural Networks},
  author = {Geuchen, Paul and Jahn, Thomas and Matt, Hannes},
  year = 2025,
  month = oct,
  journal = {Constructive Approximation},
  volume = {62},
  number = {2},
  pages = {361--402},
  issn = {1432-0940},
  doi = {10.1007/s00365-025-09713-8},
  urldate = {2026-01-20},
  abstract = {We study the universality of complex-valued neural networks with bounded widths and arbitrary depths. Under mild assumptions, we give a full description of those activation functions \$\$\textbackslash varrho :\textbackslash mathbb \textbraceleft C\textbraceright\textbackslash rightarrow \textbackslash mathbb \textbraceleft C\textbraceright\$\$that have the property that their associated networks are universal, i.e., are capable of approximating continuous functions to arbitrary accuracy on compact domains. Precisely, we show that deep narrow complex-valued networks are universal if and only if their activation function is neither holomorphic, nor antiholomorphic, nor \$\$\textbackslash mathbb \textbraceleft R\textbraceright\$\$-affine. This is a much larger class of functions than in the dual setting of arbitrary width and fixed depth. Unlike in the real case, the sufficient width differs significantly depending on the considered activation function. We show that a width of \$\$2n+2m+5\$\$is always sufficient and that in general a width of \$\$\textbackslash max \textbackslash left\textbackslash\textbraceleft{} 2n,2m\textbackslash right\textbackslash\textbraceright{} \$\$is necessary. We prove, however, that a width of \$\$n+m+3\$\$suffices for a rich subclass of the admissible activation functions. Here, n and m denote the input and output dimensions of the considered networks. Moreover, for the case of smooth and non-polyharmonic activation functions, we provide a quantitative approximation bound in terms of the depth of the considered networks.},
  langid = {english},
  keywords = {30E10,31A30,41A30,41A63,68T07,Complex-valued neural networks,Holomorphic function,Polyharmonic function,Uniform approximation,Universality},
  file = {/home/kavi/Zotero/storage/KHUFWABF/Geuchen et al. - 2025 - Universal approximation with complex-valued deep narrow neural networks.pdf}
}

@article{kimInvestigatingImpactData2025,
  title = {Investigating the Impact of Data Normalization Methods on Predicting Electricity Consumption in a Building Using Different Artificial Neural Network Models},
  author = {Kim, Yang-Seon and Kim, Moon Keun and Fu, Nuodi and Liu, Jiying and Wang, Junqi and Srebric, Jelena},
  year = 2025,
  month = jan,
  journal = {Sustainable Cities and Society},
  volume = {118},
  pages = {105570},
  issn = {2210-6707},
  doi = {10.1016/j.scs.2024.105570},
  urldate = {2026-01-20},
  abstract = {The study investigates the impact of data normalization on the prediction of electricity consumption in buildings using four multilayer Artificial Neural Networks (ANN) algorithms: Long Short-Term Memory Networks (LSTM), Levenberg--Marquardt Back-propagation (LMBP), Recurrent Neural Networks (RNN), and General Regression Neural Network (GRNN). Four data normalization approaches, Min-Max Scaling, Mean, Z-score, and Gaussian function were assessed on experimental datasets. The LSTM algorithm, when combined with Min-Max normalization, showed the most favorable predictive capabilities, with a low Coefficient of Variation of the Root Mean Square Error (CVRMSE) of 10.3 and Normalized Mean Bias Error (NMBE) of 0.6. The remaining three normalization approaches showed satisfactory concordance with empirical data, but with slight disparities in precision. The LMBP model, when using Z-score normalization, had favorable performance in forecasting electricity consumption, but the discrepancies across the models were not significant. The Recurrent Neural Network (RNN) model, when used with Gaussian normalization, exhibited the most favorable performance, with the lowest Coefficient of Variation of Root Mean Square Error (CVRMSE) at 11.8 and Normalized Mean Biased Error (NMBE) at 0.6. The Generalized Regression Neural Network (GRNN) model, trained on unprocessed data, exhibited superior performance, with the lowest Coefficient of Variation of Root Mean Square Error (CVRMSE) at 19.2 and NMBE at 1.0. In conclusion, the study highlights the significant influence of data normalization on the predictive capabilities of various ANN models, suggesting that careful use of data normalization techniques can significantly improve the accuracy of electricity consumption forecasting in buildings.},
  keywords = {Artificial neural networks,Data normalization,Electricity prediction,Occupancy rates}
}

@article{kumaryogeshMachineLearningApproach2025,
  title = {A Machine Learning Approach for Estimating Supercapacitor Performance of Graphene Oxide Nano-Ring Based Electrode Materials},
  author = {Kumar~Yogesh, Gaurav and Nandi, Debabrata and Yeetsorn, Rungsima and Wanchan, Waritnan and Devi, Chandni and Pratap~Singh, Ravi and Vasistha, Aditya and Kumar, Mukesh and Koinkar, Pankaj and Yadav, Kamlesh},
  year = 2025,
  journal = {Energy Advances},
  volume = {4},
  number = {1},
  pages = {119--139},
  publisher = {Royal Society of Chemistry},
  doi = {10.1039/D4YA00577E},
  urldate = {2026-01-20},
  langid = {english},
  file = {/home/kavi/Zotero/storage/CSKWR2IL/KumarÂ Yogesh et al. - 2025 - A machine learning approach for estimating supercapacitor performance of graphene oxide nano-ring ba.pdf}
}

@inproceedings{madabattulaModelingSupercapacitor2012,
  title = {Modeling of {{Supercapacitor}}},
  booktitle = {Proceedings of the 2012 {{COMSOL Conference}}},
  author = {Madabattula, Ganesh and Gupta, Sanjeev K.},
  year = 2012,
  publisher = {COMSOL},
  address = {Bangalore},
  file = {/home/kavi/Zotero/storage/C3CLXQMV/Madabattula and Gupta - 2012 - Modeling of Supercapacitor.pdf}
}

@inproceedings{mouDropoutTrainingDatadependent2018,
  title = {Dropout {{Training}}, {{Data-dependent Regularization}}, and {{Generalization Bounds}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Mou, Wenlong and Zhou, Yuchen and Gao, Jun and Wang, Liwei},
  year = 2018,
  month = jul,
  pages = {3645--3653},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2026-01-21},
  abstract = {We study the problem of generalization guarantees for dropout training. A general framework is first proposed for learning procedures with random perturbation on model parameters. The generalization error is bounded by sum of two offset Rademacher complexities: the main term is Rademacher complexity of the hypothesis class with minus offset induced by the perturbation variance, which characterizes data-dependent regularization by the random perturbation; the auxiliary term is offset Rademacher complexity for the variance class, controlling the degree to which this regularization effect can be weakened. For neural networks, we estimate upper and lower bounds for the variance induced by truthful dropout, a variant of dropout that we propose to ensure unbiased output and fit into our framework, and the variance bounds exhibits connection to adaptive regularization methods. By applying our framework to ReLU networks with one hidden layer, a generalization upper bound is derived with no assumptions on the parameter norms or data distribution, with O(1/n)O(1/n)O(1/n) fast rate and adaptivity to geometry of data points being achieved at the same time.},
  langid = {english},
  file = {/home/kavi/Zotero/storage/9KJBJ5PI/Mou et al. - 2018 - Dropout Training, Data-dependent Regularization, and Generalization Bounds.pdf}
}

@article{ravichandranMachineLearningBasedPrediction2024,
  title = {Machine {{Learning-Based Prediction}} of {{Cyclic Voltammetry Behavior}} of {{Substitution}} of {{Zinc}} and {{Cobalt}} in {{BiFeO3}}/{{Bi25FeO40}} for {{Supercapacitor Applications}}},
  author = {Ravichandran, Abhilash and Raman, Valliappan and Selvaraj, Yogapriya and Mohanraj, Prabhavathy and Kuzhandaivel, Hemalatha},
  year = 2024,
  month = aug,
  journal = {ACS Omega},
  volume = {9},
  number = {31},
  pages = {33459--33470},
  publisher = {American Chemical Society},
  doi = {10.1021/acsomega.3c10485},
  urldate = {2026-01-20},
  abstract = {Artificial intelligence and machine learning have become indispensable tools across various disciplines in the present century. In that way, the role of artificial intelligence and machine learning in energy storage devices was investigated. As a preliminary study, the data derived from electrochemical studies were used for the prediction. The prediction of current from cyclic voltammetry (CV) studies was undertaken for bismuth ferrite (BFO), substitution of zinc in BFO (BFZO), and substitution of cobalt in BFO composite (BFCO). CV is a vital electrochemical technique used for studying the electrochemical behavior of any material. The electrochemical study provides insights into the energy storage behavior of the material through the specific capacitance. The machine learning models, such as Artificial Neural Network (ANN), Random Forest (RF), and XGBoost (XGB), are trained and implemented to predict current at different scan rates. These models are trained and validated using the data collected from a CHI 600E electrochemical workstation. Multiple trials of experiments were performed to build the most optimum model for the material. The predicted values provide promising results and align well with the experimental data. The XGBoost, ANN and RF models perform well for the CV data set with an average testing accuracy {$>$}97\%. Also, a meta-model was created using stacking of the above three machine learning models which further improved the predictive performance, achieving a slightly higher average testing accuracy of over 97.73\%. The outcomes from the models can promote the development of machine learning applications in the field of electrochemistry and provide insights into optimizing supercapacitor performance and design through data-driven approaches.},
  file = {/home/kavi/Zotero/storage/QCUES46N/Ravichandran et al. - 2024 - Machine Learning-Based Prediction of Cyclic Voltammetry Behavior of Substitution of Zinc and Cobalt.pdf}
}

@article{srivastavaDropoutSimpleWay2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  shorttitle = {Dropout},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = 2014,
  journal = {Journal of Machine Learning Research},
  volume = {15},
  number = {56},
  pages = {1929--1958},
  issn = {1533-7928},
  urldate = {2026-01-21},
  langid = {english},
  file = {/home/kavi/Zotero/storage/NK6M3SPN/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf}
}
